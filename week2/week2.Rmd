---
title: "week2"
author: "Mathias Svendsen"
date: "November 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(gridExtra)
library(cowplot)
```



```{r}
setwd("~/School/Master in statistics/Sbe/week2")
cells = read.csv("cell_types.csv", na.strings = "")
```

**EX 4.1**

```{r}
#1
left = cells$specimen__hemisphere == "left"
right = cells$specimen__hemisphere == "right"

rst = cells$ef__peak_t_ramp # ramp spike time
rst_left = na.omit(rst[left]) # leave out missing values
rst_right = na.omit( rst[right]) # leave out missing values

par(mfrow  = c(1,2))

hist(rst_left,probability = T
     , main = 'Rst (left  hemisphere)', xlab = 'Ramp spike time' )

hist(rst_right, probability = T
     , main = 'Rst (right  hemisphere)', xlab = 'Ramp spike time' )

```

The distributions look quite similar with a right-skewed shape.

**EX 4.2**

We use the build in `density` function with a gaussian kernal to provide kernal density estimates for rsp (ramp spike time) for left and right hemisphere respectively. 

```{r}

kde_rst_left = density(rst_left)
kde_rst_right = density(rst_right)
plot(kde_rst_left$x, kde_rst_left$y,
     lwd = 2,
     type = 'l',
     col = "#0000CD",
     main = "Kde's (blue: rst_left), (red: rst_right)",
     xlab = 'ramp spike time',
     ylab = 'density')
lines(kde_rst_right, col = '#c91246', lwd = 2)





```

Compared to the histograms, it is easier to notice differences from the kernal density estimates (the blue curve is slightly more smooth). However as the histograms illustrated, the differences are small.


**EX 4.3**

```{r}

homo_sapiens = subset(cells, donor__species == 'Homo Sapiens')
male = homo_sapiens$donor__sex == 'Male'
female = homo_sapiens$donor__sex == 'Female'

rst_male = na.omit(homo_sapiens$ef__peak_t_ramp[male])
rst_female = na.omit(homo_sapiens$ef__peak_t_ramp[female])


kde_rst_male = density(rst_male)
kde_rst_female = density(rst_female)

plot(kde_rst_male$x, kde_rst_male$y,
     lwd = 2,
     type = 'l',
     col = "#0000CD",
     main = "Kde's (blue: rst_male), (red: rst_female)",
     xlab = 'ramp spike time',
     ylab = 'density')
lines(kde_rst_female, col = '#c91246', lwd = 2)




```

Again both of the kernal density estimates look quite similar (same right-skewed shape), only with small differences.


**EX 5**

The parameters for the lognormal distribution that appeared to fit the ramp spike time distribution the best was found to be 

```{r}

x = sort(na.omit(cells$ef__peak_t_ramp)) # sorted data (empirical quantiles)

# MLE of mean and std for x (lognormal distribution)
meanlog = mean(log(x)) 
sdlog = sd(log(x))

hist(x, probability = T, xlab = "ramp spike time")
curve(dlnorm(x, sdlog = sdlog, meanlog = meanlog), add = TRUE, 
        from = 1, to = 30,
        col= 8*meanlog)
```


We can produce a QQ-plot to compare the distributions (the assumed distribution, vs theoretical distribution).

```{r}
p = c() 
n = length(x)
 
for (i in 1:n) {
  p[i] = (i-0.5)/n
}

plot(qlnorm(p, sdlog = sdlog, meanlog = meanlog) ~ x,
     main = "QQ-plot", xlab = "empirical quantiles (rst)", ylab = "theoretical quantiles");abline(0,1,lwd = 2, col = "darkgreen")
     
                                                            
```


Except for a few observations (large values of rst) the points approximately follow the straight line y = x, indicating that the data was indeed generated by the lognormal distriution with meanlog and sdlog.


**EX 5.2**

```{r}
hist(x, probability = T, xlab = "ramp spike time")
curve(dnorm(x, sd = 4.32, mean = 6.41), add = TRUE, 
        from = 1, to = 30,
        col= 6*meanlog)
```

The distribution of rst is obvioulsy right-skewed, which cant be captured by a normal distribution.

**5.3**

```{r}
plot(qnorm(p, sd = 4.32, mean = 6.41) ~ x,
     main = "QQ-plot", xlab = "empirical quantiles (rst)", ylab = "theoretical quantiles");abline(0,1,lwd = 2, col = "darkgreen")
     
```

We see that the points deviates substantially from the straight line, and we soundly reject the hypothesis that these data should be generated from the gaussian distribution with sd = 4.32, and mean = 6.41.



**6.1**

```{r}

size = 100
p = 0.3
n_exp = 10 * 10^(seq(0,3,1))


mean_sample = c()
var_sample = c()
sd_sample = c()

mean_true = size*p
var_true = size*p*(1-p)
sd_true = sqrt(size*p*(1-p))



for (i in 1:4) {
  tmp = rbinom(n_exp[i], size = size, prob = p)
  mean_sample[i] = mean(tmp)
  var_sample[i] = var(tmp)
  sd_sample[i] = sd(tmp)
} 
  

mean_diff = abs(mean_sample - mean_true)
var_diff = abs(var_sample - var_true)
sd_diff = abs(sd_sample - sd_true)


```




```{r}
# create a dataset

condition = c('n = 10', 'n = 100', 'n = 1000', 'n = 10000')
measures = c(rep("mean_diff", 4), rep("var_diff", 4), rep("sd_diff", 4))
value = c(mean_diff, var_diff, sd_diff)
data <- data.frame(condition,measures,value)
 
# Grouped
ggplot(data, aes(fill=condition, y=value, x=measures)) + 
    geom_bar(position="dodge", stat="identity") +
    labs(y = "absolute difference")
```

The barplot displays the absolute differences between the true and empircal measures (i.e mean, variance and standard deviation) for n = 10, 100, .., 10000 respectively. It is clear to see that as n increases, the absolute difference starts to reduce.



**6.2**


```{r}
n_exp = 10000
n = 1000 # number of times to repeat experiment of rbinom(10000, size =100, prob = .3)

means_samples = c()
vars_samples = c()
sds_samples = c()

for (i in 1:n) {
  
  tmp = rbinom(n_exp, size = size, prob = p)
  means_samples[i] = mean(tmp)
  vars_samples[i] = var(tmp)
  sds_samples[i] = sd(tmp)
  
}


means_samples = as.data.frame(means_samples)
vars_samples = as.data.frame(vars_samples)
sds_samples = as.data.frame(sds_samples)


p1 =  ggplot(means_samples, aes(means_samples)) +
  geom_histogram(color = "black", fill = "white", alpha = I(.2)) + 
  xlab("mean value") +
  ylab("count") +
  geom_vline(xintercept = mean_true, color = "red", lwd = 2)

p2 = ggplot(vars_samples, aes(vars_samples)) +
  geom_histogram(color = "black", fill = "white", alpha = I(.2)) + 
  xlab("var value") +
  ylab("count") +
  geom_vline(xintercept = var_true, color = "blue", lwd = 2)

p3 = ggplot(sds_samples, aes(sds_samples)) +
  geom_histogram(color = "black", fill = "white", alpha = I(.2)) + 
  xlab("sd value") +
  ylab("count") +
  geom_vline(xintercept = sd_true, color = "green", lwd = 2)  
  
```


```{r}
grid.arrange(p1, p2, p3, ncol = 2, top = "Histograms of means, vars and sds (n = 1000)")
```


The figure displays histograms of 1000 empirical means, variances and standard deviations computed from the experiment of flipping a coin 10.000 times where the probability of heads (succes) is 0.3 . We notice how these empirical distributions looks normally distributed with means located around the true values represented by vertical lines (i.e true mean variance and standard deviation).  


**EX 6.3**

```{r}
sd_mean_empircal = sd(means_samples$means_samples)
sd_mean = sd_true/sqrt(n_exp) 

abs(sd_mean_empircal - sd_mean)
```

the absolute difference between the standard error of the mean and the empirical standard error of the means are almost zero, which implies that the empirical value is a quite good estimate of the true value. Increasing n would yiled an even better estimate of the true vale (say n = n*10).



**EX 6.4**

```{r}
ggplot(means_samples, aes(means_samples)) +
  geom_histogram(aes(y = ..density..), color = "black", fill = "white", alpha = I(.2)) +
  geom_area(stat = "function", fun = dnorm, args = list(mean = mean_true, sd = sd_mean ), fill = "darkred",
            alpha = I(.2), color = "darkorange")  
```


The figure displyas the histogram of the 1000 sampled means along with the pdf from the true distribution. We see that the density curve approximates the histogram quite well (increasing n would produce an even nicer figure) 






